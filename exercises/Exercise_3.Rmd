---
title: "Exercise Chapter 3"
author: "Tobias Hugentobler"
date: "14.03.2023"
output:
  html_document:
    toc: yes
    toc_float: yes
    df_print: paged
    theme: paper
    code_folding: show
    math_method: katex
---
```{r include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE,
  message = FALSE, 
  fig.height = 5,
  fig.width = 6,
  eval = TRUE
)
```

## Exercise 1

Use the diamonds data to fit a linear regression to model expected price (without logarithm) as a function of "carat" (no log), "color", "clarity", and "cut". Interpret the output of the model. Does it make sense?

For the first four exercises, start with this snippet to turn the ordered factors into unordered ones.

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)

diamonds <- mutate_if(diamonds, is.ordered, factor, ordered = FALSE)
```


```{r}
model1 <- lm(price ~ carat + color + clarity + cut, diamonds)
summary(model1)
```
The most factors makes sense, but the Color is a bit odd. The worse the color the lower the price, the better the cut, the higher the price. R^2 is 91%, that's not bad

## Exercise 2

Try to improve the model from Exercise 1 by adding interactions between our main predictor "carat" and "color", between "carat" and "cut", and also between "carat" and "clarity". Why could this make sense? How many additional parameters are required? How does the RMSE react?

```{r}
model2 <- lm(price ~ carat * (color + clarity + cut), diamonds)
summary(model2)
```
Params: for each effect of color, clarity an d cut we get a "carat" effect
RSME: Goes down from 1157 to 1033

## Exercise 3

In the regression in Exercise 1, represent "carat" by a restricted cubic spline with four knots. What is a restricted cubic spline? How much better does the RMSE get? Visualize the effect of "carat". Hint: Restricted cubic splines are available in the R package "splines". 

```{r}
library(splines)
model_splines <- lm(price ~ ns(carat, df = 5) + color + cut + clarity, diamonds)
summary(model_splines)
```

Not much better than the model 1, b/c the dependency of carat is almost linear

## Exercise 4

Fit a Gamma regression with log-link to explain diamond prices by "log(carat)", "color", "cut", and "clarity". Compare the coefficients with those of a linear regression having the same covariates, but using "log(price)" as response. Calculate the relative bias of the average prediction. Why isn't it 0?

```{r}
fit <- glm(
  price ~ log(carat) + color + cut + clarity,
  data = diamonds, 
  family = Gamma(link = "log")
)
summary(fit) #one can use a log(price) linear model instead of a gamma with link "log"
mean(fitted(fit)) / mean(diamonds$price) -1

fit2 <- lm(log(price) ~ log(carat) + color + cut + clarity, data = diamonds)
summary(fit2)

mean(predict(fit, type = "response")) / mean(diamonds$price) - 1

mean(exp(fitted(fit2))) / mean(diamonds$price) - 1
contrasts(diamonds$color) #only for kategorical, rows factorlevel, and columns are dummies
```
The coefficients are almost the same. The relative bias of the average prediction is slightly lower than zero, and that's because the prediction has a small error in them, we don't explain the model perfectly. The R squares are very high, therefore the coefficients between the models are similar

## Exercise 5

Fit the Gamma GLM of Exercise 4 with H2O, optionally replacing the data preparation with "data.table". Do you get the same results?

```{r}
library(arrow)
library(data.table)
library(h2o)

h2o.init(min_mem_size = "6G")

diamonds$log_carat = log(diamonds$carat)
# diamonds$log_price = log(diamonds$price)
h2o_df <- as.h2o(diamonds)

summary(h2o_df)

x = c("log_carat", "color", "cut", "clarity")

fit_h2o <- h2o.glm(
    x, "price", 
    family = "gamma", 
    training_frame = h2o_df, 
    compute_p_values = TRUE, 
    lambda = 0, #no penalty
    link = "log"
  )
fit_h2o
```
Up to rounding errors, its the same output.
